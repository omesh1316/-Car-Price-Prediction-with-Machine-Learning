{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a7560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and path setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "DATA_PROCESSED = ROOT / 'data' / 'processed'\n",
    "MODELS = ROOT / 'models'\n",
    "OUTPUTS = ROOT / 'outputs'\n",
    "RESULTS = OUTPUTS / 'results'\n",
    "GRAPHS = OUTPUTS / 'graphs'\n",
    "for p in [DATA_PROCESSED, MODELS, RESULTS, GRAPHS]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "X_TEST_IN = DATA_PROCESSED / 'X_test.csv'\n",
    "Y_TEST_IN = DATA_PROCESSED / 'y_test.csv'\n",
    "MODEL_IN = MODELS / 'car_price_model.pkl'\n",
    "METRICS_OUT = RESULTS / 'evaluation_metrics.csv'\n",
    "REPORT_OUT = RESULTS / 'model_evaluation_report.txt'\n",
    "PREDICTIONS_OUT = RESULTS / 'evaluation_predictions.csv'\n",
    "PLOT_PRED_ACT = GRAPHS / 'eval_predicted_vs_actual.png'\n",
    "PLOT_RESID = GRAPHS / 'eval_residuals_hist.png'\n",
    "PLOT_CAL = GRAPHS / 'eval_calibration.png'\n",
    "\n",
    "print('Evaluation inputs:', X_TEST_IN, Y_TEST_IN, MODEL_IN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd724a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and test data with error handling\n",
    "try:\n",
    "    with open(MODEL_IN, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    sys.exit(f'ERROR: Model file not found at {MODEL_IN}. Run training first.')\n",
    "except Exception as e:\n",
    "    sys.exit(f'ERROR loading model: {e}')\n",
    "\n",
    "try:\n",
    "    X_test = pd.read_csv(X_TEST_IN)\n",
    "    y_test = pd.read_csv(Y_TEST_IN).squeeze()\n",
    "except FileNotFoundError as e:\n",
    "    sys.exit(f'ERROR: Test dataset missing: {e}')\n",
    "except Exception as e:\n",
    "    sys.exit(f'ERROR reading test data: {e}')\n",
    "\n",
    "print('Loaded model and test data shapes:', getattr(X_test, 'shape', None), getattr(y_test, 'shape', None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509c9d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions and metrics\n",
    "try:\n",
    "    preds = model.predict(X_test)\n",
    "except Exception as e:\n",
    "    sys.exit(f'ERROR during prediction: {e}')\n",
    "\n",
    "# Ensure arrays\n",
    "y_true = np.asarray(y_test).ravel()\n",
    "y_pred = np.asarray(preds).ravel()\n",
    "\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "# Avoid division by zero in MAPE\n",
    "mape = np.mean(np.abs((y_true - y_pred) / np.where(y_true==0, 1e-8, y_true))) * 100\n",
    "\n",
    "metrics_df = pd.DataFrame([{\n",
    "    'mae': mae,\n",
    "    'rmse': rmse,\n",
    "metrics_df.to_csv(METRICS_OUT, index=False)\n",
    "print('Saved evaluation metrics to', METRICS_OUT)\n",
    "display(metrics_df)\n",
    "\n",
    "# Save predictions with true values\n",
    "pred_df = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred})\n",
    "pred_df.to_csv(PREDICTIONS_OUT, index=False)\n",
    "print('Saved predictions to', PREDICTIONS_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f944b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots: Predicted vs Actual, Residuals, Calibration (binned)\n",
    "try:\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.6)\n",
    "    mn = min(y_true.min(), y_pred.min())\n",
    "    mx = max(y_true.max(), y_pred.max())\n",
    "    plt.plot([mn, mx], [mn, mx], 'r--')\n",
    "    plt.xlabel('Actual Selling_Price')\n",
    "    plt.ylabel('Predicted Selling_Price')\n",
    "    plt.title('Predicted vs Actual (Test)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOT_PRED_ACT, dpi=150)\n",
    "    plt.show()\n",
    "    print('Saved', PLOT_PRED_ACT)\n",
    "\n",
    "    residuals = y_true - y_pred\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.hist(residuals, bins=40, edgecolor='k')\n",
    "    plt.title('Residuals Distribution (Test)')\n",
    "    plt.xlabel('Residual')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOT_RESID, dpi=150)\n",
    "    plt.show()\n",
    "    print('Saved', PLOT_RESID)\n",
    "\n",
    "    # Simple calibration plot: bin by predicted value and plot mean(true) vs mean(pred)\n",
    "    bins = pd.qcut(y_pred, q=10, duplicates='drop')\n",
    "    calib = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred, 'bin': bins})\n",
    "    calib_group = calib.groupby('bin').agg({'y_true': 'mean', 'y_pred': 'mean'}).reset_index()\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(calib_group['y_pred'], calib_group['y_true'], 'o-')\n",
    "    plt.plot([calib_group['y_pred'].min(), calib_group['y_pred'].max()], [calib_group['y_pred'].min(), calib_group['y_pred'].max()], 'r--')\n",
    "    plt.xlabel('Mean Predicted')\n",
    "    plt.ylabel('Mean Actual')\n",
    "    plt.title('Calibration by Predicted Quantile')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOT_CAL, dpi=150)\n",
    "    plt.show()\n",
    "    print('Saved', PLOT_CAL)\n",
    "except Exception as e:\n",
    "    print('Could not create/save evaluation plots:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d54c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to save feature importances if model exposes them and feature names are available\n",
    "try:\n",
    "    if hasattr(model, 'feature_importances_') and hasattr(X_test, 'columns'):\n",
    "        fi = model.feature_importances_\n",
    "        fi_df = pd.DataFrame({'feature': X_test.columns.tolist(), 'importance': fi}).sort_values('importance', ascending=False)\n",
    "        fi_df.to_csv(RESULTS / 'evaluation_feature_importances.csv', index=False)\n",
    "        print('Saved feature importances to', RESULTS / 'evaluation_feature_importances.csv')\n",
    "except Exception as e:\n",
    "    print('Could not save feature importances:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324455dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a simple human-readable report\n",
    "try:\n",
    "    with open(REPORT_OUT, 'w', encoding='utf-8') as f:\n",
    "        f.write('Model Evaluation Report\n",
    "')\n",
    "        f.write('====================\n",
    "')\n",
    "        f.write(f'R2: {r2:.4f}\n",
    "')\n",
    "        f.write(f'MAE: {mae:.4f}\n",
    "')\n",
    "        f.write(f'MSE: {mse:.4f}\n",
    "')\n",
    "        f.write(f'RMSE: {rmse:.4f}\n",
    "')\n",
    "        f.write(f'MAPE (%): {mape:.4f}\n",
    "')\n",
    "        f.write('\n",
    "Notes: All paths are relative. For more details see evaluation_metrics.csv and evaluation_predictions.csv')\n",
    "    print('Saved evaluation report to', REPORT_OUT)\n",
    "except Exception as e:\n",
    "    print('Could not write report file:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9d50dd",
   "metadata": {},
   "source": [
    "## End of Evaluation\n",
    "\n",
    "Saved outputs:\n",
    "- `outputs/results/evaluation_metrics.csv`\n",
    "- `outputs/results/evaluation_predictions.csv`\n",
    "- `outputs/results/model_evaluation_report.txt`\n",
    "- `outputs/graphs/` plots: `eval_predicted_vs_actual.png`, `eval_residuals_hist.png`, `eval_calibration.png`\n",
    "- (optional) `outputs/results/evaluation_feature_importances.csv` if available from model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
